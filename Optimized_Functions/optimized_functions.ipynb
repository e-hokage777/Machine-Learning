{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd586a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "from itertools import combinations_with_replacement as combs_r\n",
    "from itertools import combinations as combs\n",
    "\n",
    "########################################################################################################\n",
    "############################### LINEAR REGRESSION FUNCTIONS ############################################\n",
    "########################################################################################################\n",
    "\n",
    "####                    OPTIMIZTED GRADIENT DESCENT FUNCTION                   ####\n",
    "def gradientDescent(X,y,theta, num_iters, alpha):\n",
    "    m = X.shape[0]\n",
    "    costs = np.zeros((num_iters,1));\n",
    "    thetas = np.zeros((num_iters,theta.size));\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        temp_thetas = (theta - alpha * ((np.dot(X.transpose(),(np.dot(X,theta)-y)))/m))\n",
    "\n",
    "        theta = temp_thetas\n",
    "        thetas[i] = temp_thetas.reshape(1,temp_thetas.size)\n",
    "        costs[i] = calculateCost(X,y,theta)\n",
    "        \n",
    "        # plotting costs against the number of iterations\n",
    "        # to check if gradient descent is working properly\n",
    "        fig = plt.figure(figsize=(12,6))\n",
    "        plt.title(\"Costs against number of iterations\")\n",
    "        plt.ylabel(\"Costs\")\n",
    "        plt.xlabel(\"Number of iterations\")\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return {\"hypothesis_theta\":theta, \"costs\": costs, \"thetas\":thetas}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    OPTIMIZTED FEATURE NORMALIZATION FUNCTION                   ####\n",
    "def featureNormalize(features):\n",
    "    #Getting necessary variables\n",
    "    means = features.mean(axis = 0)\n",
    "    stds = features.std(axis = 0)\n",
    "\n",
    "    featuresNorm = (features-means)/stds\n",
    "            \n",
    "    \n",
    "    return {\"normalized_features\":featuresNorm, \"means\":means, \"stds\":stds}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    OPTIMIZTED FEATURE ADDITION FUNCTION                  ####\n",
    "\n",
    "def addFeatures(X, num_combinations):\n",
    "    if(num_combinations == 1 or num_combinations == 0):\n",
    "        return X\n",
    "    \n",
    "    new_X = X\n",
    "    num_features = X.shape[1]\n",
    "    num_training_examples = X.shape[0]\n",
    "    n = num_features-1\n",
    "    #finding the number of permutations of the features\n",
    "   \n",
    "    \n",
    "    for c in range(2, num_combinations+1):\n",
    "        feature_combinations = combs_r(range(num_features),c)\n",
    "        for feature_combination in feature_combinations:\n",
    "            accumulator = 1\n",
    "            for i in feature_combination:\n",
    "                accumulator *= X[:, i]\n",
    "            \n",
    "            n+=1\n",
    "            new_X = np.insert(new_X,n, accumulator,axis=1)\n",
    "        \n",
    "    return new_X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    NORMAL EQUATION FUNCTION                  ####\n",
    "def normalEquation(X,y):\n",
    "    X_t = X.transpose();\n",
    "    hypothesis_theta = np.dot(np.linalg.pinv(np.dot(X_t,X)),np.dot(X_t,y))\n",
    "    return hypothesis_theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    REGULARIZED NORMAL EQUATION FUNCTION                  ####\n",
    "def normalEquationReg(X,y,l):\n",
    "    X_t = X.transpose();\n",
    "    X_Xt = np.dot(X_t,X)\n",
    "    identity = np.eye(X_Xt.shape[0],X_Xt.shape[1])\n",
    "    identity[0,0] = 0\n",
    "    hypothesis_theta = np.dot(np.linalg.pinv(X_Xt + l*(identity)),np.dot(X_t,y))\n",
    "    return hypothesis_theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    OPTIMIZED FUNCTION FOR CROSS VALIDATION                  ####\n",
    "def crossValidate(X,y, X_cv, y_cv, num_degrees, title, ylabel, start=1):\n",
    "    #setting up variable to hold final costs and degrees arrays\n",
    "    costs = np.zeros(num_degrees)\n",
    "    costs_cv = np.zeros(num_degrees)\n",
    "    degrees = np.zeros(num_degrees)\n",
    "    \n",
    "    ##Testing for each of degrees in range num_degrees\n",
    "    for i in range(start, num_degrees+1):\n",
    "        temp_X = addFeatures(temp_X, i)\n",
    "        temp_X = np.insert(temp_X, 0, 1, axis=1)\n",
    "        temp_X_cv = addFeatures(temp_X_cv, i)\n",
    "        temp_X_cv = np.insert(temp_X_cv, 0, 1, axis=1)\n",
    "        \n",
    "        theta = normalEquation(temp_X, y)\n",
    "        costs[i-1] = calculateCost(temp_X, y, theta)\n",
    "        costs_cv[i-1] = calculateCost(temp_X_cv, y_cv, theta)\n",
    "        degrees[i-1] = i\n",
    "        \n",
    "    ##plotting the costs against degrees for both CV data and Training data\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Degrees\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(degrees,costs, label=\"Training data\")\n",
    "    plt.plot(degrees,costs_cv, label=\"Cv data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ##Obtaining and returning the degree that produced the minimum cost CV data\n",
    "    min_cost = min(costs_cv)\n",
    "    min_degree = np.array(np.where(costs_cv == min_cost))[0][0] + 1\n",
    "    \n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    result[\"min_cost\"] = min_cost\n",
    "    result[\"min_degree\"] = min_degree\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                    OPTIMIZED FUNCTION FOR FINDING OPTIMAL LAMBDA                  ####\n",
    "## Function for finding optiaml lambda\n",
    "def optimalLambda(X, y, X_cv, y_cv, lambdas, degree, title, ylabel):\n",
    "    size_lambdas = lambdas.size\n",
    "    costs = np.zeros(size_lambdas)\n",
    "    costs_cv = np.zeros(size_lambdas)\n",
    "    temp_X = addFeatures(X,degree)\n",
    "    temp_X_cv = addFeatures(X_cv,degree)\n",
    "    temp_X = np.insert(temp_X,0,1,axis=1)\n",
    "    temp_X_cv = np.insert(temp_X_cv,0,1,axis=1)\n",
    "    \n",
    "    for i in range(size_lambdas):\n",
    "        theta = normalEquationReg(temp_X,y,lambdas[i])\n",
    "        costs[i] = calculateCost(temp_X,y,theta)\n",
    "        costs_cv[i] = calculateCost(temp_X_cv,y_cv,theta)\n",
    "        \n",
    "    ##plotting the costs against lambdas for both CV data and Training data\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Lambdas\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(lambdas,costs, label=\"Training data\")\n",
    "    plt.plot(lambdas,costs_cv, label=\"Cv data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ##Obtaining and returning the degree that produced the minimum cost CV data\n",
    "    min_cost = min(costs_cv)\n",
    "    min_lambda = lambdas[np.array(np.where(costs_cv == min_cost))[0][0]]\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    result[\"min_cost\"] = min_cost\n",
    "    result[\"min_lambda\"] = min_lambda\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "####                    FUNCTION FOR SEPARATING DATA INTO TRAINING, CROSS VALIDATION AND TEST                  ####\n",
    "### -----NB: the data also returns the training data size\n",
    "\n",
    "def trs_cvs_ts(X):\n",
    "    \n",
    "    ## !!! HAVE TO IMPORT math MODULE BEFORE RUNNING THIS FUNCTION\n",
    "    \n",
    "    num_training_egs = X.shape[0]\n",
    "    training_data_size = 0.6 * num_training_egs\n",
    "    cvs_ts_size = 0.2 * num_training_egs\n",
    "    \n",
    "    tr = 0\n",
    "    cv = 0\n",
    "    \n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "#     #checking if flooring all would equal the number of training egs\n",
    "#     if (int(training_data_size) + 2 * math.ceil(cvs_ts_size)) == num_training_egs:\n",
    "#         tr = int(training_data_size)\n",
    "#         cv = tr + math.ceil(cvs_ts_size)\n",
    "#         ts = tr + cv + math.ceil(cvs_ts_size)\n",
    "#     elif (math.ceil(training_data) + 2 * int(cvs_ts_size)) == num_training_egs:\n",
    "#         tr = math.ceil(training_data_size)\n",
    "#         cv = tr + int(cvs_ts_size)\n",
    "#         ts = tr + cv + int(cvs_ts_size)\n",
    "#     else:\n",
    "#         tr = int(training_data_size)\n",
    "#         cv = tr + int(cvs_ts_size)\n",
    "#         ts = tr + cv + int(cvs_ts_size)\n",
    "        \n",
    "    data[\"trs\"] = X[0:tr, :]\n",
    "    data[\"cvs\"] = X[tr:tr+cv, :]\n",
    "    data[\"ts\"] = X[tr+cv:0, :]\n",
    "    data[\"trs_size\"] = tr\n",
    "    data[\"cvs_size\"]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   OPTIMIZED FUNCTION FOR CHECKING BEST SIZE AND COMBINATIONS OF AVAILABLE FEATURES                 ####\n",
    "def bestParams(X,y):\n",
    "    features_size = X.shape[1]\n",
    "    features  = range(features_size)\n",
    "    diff_combs = []\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(1,features_size+1):\n",
    "        c = cc(features,i)\n",
    "        for combo in c:\n",
    "            diff_combs.append(list(combo))\n",
    "    \n",
    "    for combo in diff_combs:\n",
    "        test_features = X[:,combo]\n",
    "        test_features = np.insert(test_features,0,1,axis=1)\n",
    "    \n",
    "        trcvts = trs_cvs_ts(test_features)\n",
    "        X_tr = trcvts[\"trs\"]\n",
    "        X_cv = trcvts[\"cvs\"]\n",
    "        X_t = trcvts[\"ts\"]\n",
    "        trs_size = trcvts[\"trs_size\"]\n",
    "        cvs_size = trcvts[\"cvs_size\"]\n",
    "        last_index = trcvts[\"last_index\"]\n",
    "        \n",
    "        y_tr = y[0:trs_size, :]\n",
    "        y_cv = y[trs_size:cvs_size, :]\n",
    "        y_t = y[cvs_size:last_index,:]\n",
    "        \n",
    "        theta = normalEquation(X_tr, y_tr)\n",
    "        \n",
    "        costs.append(calculateCost(X_cv, y_cv, theta))\n",
    "\n",
    "        \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.title(\"Test different sizes and combinations of features\")\n",
    "    plt.xlabel(\"Index of combinattion\")\n",
    "    plt.ylabel(\"Costs\")\n",
    "    plt.plot(costs, \"rx\")\n",
    "    plt.show()\n",
    "    \n",
    "    result = {}\n",
    "    min_cost = min(costs)\n",
    "    min_combo = diff_combs[costs.index(min(costs))]\n",
    "    \n",
    "    result[\"min_cost\"] = min_cost\n",
    "    result[\"min_combo\"] = min_combo\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   OPTIMIZED FUNCTION FOR TESTING DIFFERENT THETA STARTING POINTS FOR GRADIENT DESCENT                 ####\n",
    "\n",
    "def diffStartsThetas(X,y,theta_range, num_iters, alpha,num_tests):\n",
    "    X = np.insert(X,0,1,axis=1) \n",
    "    n = X.shape[1]\n",
    "    costs = np.zeros((num_tests,1))\n",
    "    opt_thetas = []\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        theta = np.random.rand(n,1) * theta_range\n",
    "        opt_theta = gradientDescent(X,y,theta,num_iters,alpha)[\"hypothesis_theta\"]\n",
    "        opt_thetas.append(opt_theta)\n",
    "        costs[i] = calculateCost(X,y,opt_theta)\n",
    "        \n",
    "     \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.title(\"Testing differnt starting points for theta\")\n",
    "    plt.xlabel(\"Indices\")\n",
    "    plt.ylabel(\"Costs\")\n",
    "    plt.plot(costs, \"rx\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(costs)\n",
    "    min_cost = min(costs)\n",
    "    min_theta = opt_thetas[np.where(costs == min_cost)[0][0]]\n",
    "    \n",
    "    result = {}\n",
    "    result[\"min_cost\"] = min_cost\n",
    "    result[\"min_theta\"] = min_theta\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "####                   OPTIMIZED FUNCTION FOR ADDING ONLY HIGHER POWER FEATURES               ####\n",
    "\n",
    "def addFeatures_higher_degrees_only(X,degree):\n",
    "    if(degree == 1 or degree == 0):\n",
    "        return X\n",
    "    \n",
    "    new_X = X\n",
    "    num_features = X.shape[1]\n",
    "    num_training_examples = X.shape[0]\n",
    "    \n",
    "    for d in range(2,degree+1):\n",
    "        new_X = np.concatenate([new_X, X**d],axis=1)\n",
    "        \n",
    "    return new_X\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "############################### LOGISTIC REGRESSION FUNCTIONS ############################################\n",
    "########################################################################################################\n",
    "\n",
    "####                   OPTIMIZED FUNCTION FOR VISUALIZING 2D DATA               ####\n",
    "def visualizeData2D(X,y,title,labelx,labely):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    figure = plt.figure(figsize=(10,10))\n",
    "    plt.xlabel(labelx)\n",
    "    plt.ylabel(labely)\n",
    "    plt.title(title)\n",
    "    \n",
    "    pos = (y == 1).flatten()\n",
    "    neg = (y == 0).flatten()\n",
    "    \n",
    "    plt.plot(X[pos,0], X[pos,1],\"g+\", label=\"y=1\", ms=10, mew=2)\n",
    "    plt.plot(X[neg,0], X[neg,1],\"ro\", label=\"y=0\", ms=10)\n",
    "        \n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "####                   SIGMOID FUNCTION               ####    \n",
    "def sigmoid(n):\n",
    "    return 1/(1 + np.exp(-n))\n",
    "\n",
    "\n",
    "\n",
    "####                   COST FUNCTION               ####\n",
    "def lrCostFunction(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(np.dot(X, theta[:,np.newaxis]))\n",
    "    J = 1/m * (-y * np.log(h) - (1-y) * (np.log(h))).sum()\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   GRADIENT FUNCTION               ####\n",
    "def lrGradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(np.dot(X,theta[:,np.newaxis]))\n",
    "    gradients = (X.transpose() * (h-y))/m\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   REGULARIZED COST FUNCTION               ####\n",
    "def lrCostFunctionReg(theta, X, y, l):\n",
    "    m = X.shape[0]\n",
    "    end = theta.size\n",
    "    theta = theta[:,np.newaxis]\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    J = (1/m) * np.sum(-y * np.log(h) - (1-y) * (np.log(1-h))) + ((l/(2*m))*np.sum(theta[1:end] ** 2)) \n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   REGULARIZED GRADIENT FUNCTION               ####\n",
    "def lrGradientReg(theta, X, y, l):\n",
    "    m = X.shape[0]\n",
    "    end = theta.size\n",
    "    theta = theta[:,np.newaxis]\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    gradients = np.dot(X.T, (h-y)) * (1/m) + ((l/m) * np.insert(theta[1:end],0,0))[:,np.newaxis]\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   FUNCTION FOR PLOTTING LINEAR DECISION BOUNDARY               ####\n",
    "def plotLinearDecisionBoundary(theta,X,y):\n",
    "    visualizeData(X[:,1:3],y)\n",
    "    plotX = np.array([min(X[:,1]) - 2, max(X[:,1]) + 2])\n",
    "    plotY = (-1/theta[2]) * (theta[1] * plotX + theta[0])\n",
    "    plt.plot(plotX, plotY, label = \"Decision boundary\")\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "####                   FUNCTION FOR PLOTTING NON-LINEAR DECISION BOUNDARY               ####\n",
    "def plotNonLinearDecisionBoundary(theta, X, y):\n",
    "    u = np.linspace(-1,1.5,50)\n",
    "    v = np.linspace(-1,1.5,50)\n",
    "    \n",
    "    z = np.zeros((u.size, u.size))\n",
    "    \n",
    "    for i in range(u.size):\n",
    "        for j in range(u.size):\n",
    "            res = optimizedfunctions.addFeatures(np.array([u[i],v[j]])[np.newaxis,:],6)\n",
    "            res = np.insert(res,0,1)\n",
    "            z[i,j] = res @ theta\n",
    "    \n",
    "    print(z.shape)\n",
    "    plt.contour(u,v,z,levels=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "########################################################################################################\n",
    "############################### CLASSIFICATION FUNCTIONS ############################################\n",
    "########################################################################################################\n",
    "\n",
    "####                   FUNCTION TRAINING ALL THETAS FOR ALL GIVEN CLASSES              ####\n",
    "def oneVsAll(X,y,num_labels,l):\n",
    "    n = X.shape[1]\n",
    "    thetas = np.zeros((num_labels, n))\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        temp_theta = (thetas[i,:])[np.newaxis,:]\n",
    "        print(\"Training...........\", end=\" \")\n",
    "        thetas[i,:], cost, end_flag = fmin_tnc(func=optimizedfunctions.lrCostFunctionReg, x0=temp_theta, args=(X,(y==i).astype(int),l), fprime=optimizedfunctions.lrGradientReg)\n",
    "        \n",
    "    return thetas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   FUNCTION FOR ONE VS ALL PREDICTION              ####\n",
    "def predictOneVsAll(all_theta, X):\n",
    "    return optimizedfunctions.sigmoid(np.dot(X, all_theta.T).max(axis=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####                   FUNCTION FOR PREDICTING ONE TEST IN CLASSIFICATION             ####\n",
    "def predict(all_theta, x):\n",
    "    return optimizedfunctions.sigmoid(np.dot(x, all_theta.T)).max()  \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################################################################\n",
    "############################### IMAGE PROCESSING FUNCTIONS ############################################\n",
    "########################################################################################################\n",
    "\n",
    "# function for visualizing images in a grid by plotting square grid\n",
    "def gridImages(array, rows, cols):\n",
    "    n = int(math.sqrt(array.shape[1]))\n",
    "    num_images = rows * cols\n",
    "    images = []\n",
    "    \n",
    "    #creating grid\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    \n",
    "    grid = ImageGrid(fig, 111, (rows, cols))\n",
    "    \n",
    "    # for creating array of image pixels\n",
    "    for i in range(num_images):\n",
    "        image = X[i,:].reshape(n,n)\n",
    "        images.append(image)\n",
    "        \n",
    "    for ax, im in zip(grid,images):\n",
    "        ax.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
